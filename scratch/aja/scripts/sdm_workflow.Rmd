---
title: "SDM Workflow for COCA I"
author: "Andrew Allyn"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: 
  html_document:
    toc: TRUE
    toc_float:
        collapsed: FALSE
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, quiet = TRUE)
```

```{r, echo  = FALSE, align = "right", height = 44}
# Logo
#knitr::include_graphics("~/GitHub/gmRi/inst/stylesheets/gmri_logo.png")
# Access GMRI CSS Style
#gmRi::insert_gmri_header(header_file = "gmri_logo_header.html")
#gmRi::use_gmri_style_rmd(css_file = "gmri_rmarkdown.css")

# Evaluate? Used for testing layout
eval_use<- FALSE
```

## Overview

## Preliminaries
A few things before the work begins. First, getting the gmRi library, which has our function for generating paths to folders on Box. Second, sourcing some functions that we need. I'm sure there is a better way of doing this.
```{r, eval = TRUE, echo = FALSE, warning = FALSE, results = "hide", message = FALSE}
library(here)
library(gmRi)
library(dtplyr)
library(tidyverse)
library(broom)
library(lubridate)
library(sf)
library(raster)
library(conflicted)
library(mgcv)
filter<- dplyr::filter
select<- dplyr::select
extract<- raster::extract
os_use<- .Platform$OS.type
source(here::here("/scratch/aja/scripts", "library_check_func.R"))
source(here::here("/scratch/aja/scripts", "nefsc_trawl_prep_func.R"))
source(here::here("/scratch/aja/scripts", "static_extract_func.R"))
source(here::here("/scratch/aja/scripts", "dynamic_2d_extract_func.R"))
source(here::here("/scratch/aja/scripts", "taylor_diagram_func.R"))
```

## The Workflow
### Processing raw NOAA NEFSC data
The first step for the workflow is to process the raw NOAA NEFSC data, which is usually provided by email, and output a "tidy" data set that has one row per observation, where the observation is defined as a unique tow/trawl - species biomass caught. To complete this processing, I use the `nefsc_dat_prep_func.R` file. This is good in some ways, but bad in others (some hard wiring of strata to delete). Also, right now it works over all species and this means it can be a bit slow. *We could think about adding an argument that includes a vector of species names?*

**Input**  
    +  File path to raw NOAA NEFSC trawl .Rdata file.  

**Output**  
    +  A tidy data set, where each row is a unique tow-species observation.  

**Other arguments**  
    +  out_path = Path to where we want to save the new file.  

```{r, hide = TRUE, eval = eval_use}
survdat_path<- paste(shared.path(os.use = os_use, group = "root", folder = "RES Data/NMFS_trawl/"), "Survdat_Nye_allseason.Rdata", sep = "")
nefsc_dat<- nefsc_trawl_prep(survdat_path = survdat_path, out_path = here::here("/scratch/aja/data/"))
```

### Collecting model covariates
The second step for the workflow is to extract covariates of interest at each of the unique tow locations within the tidy trawl data set (`nefsc_dat`). I've done this a couple different ways. At one point, I had this baked right into the trawl data processing code. Now, though, I am trying to use a specific environmental data extraction function. A few things to note about this extraction workflow before getting to the function or functions. To start, we are usually interested in some combination of static covariates (e.g., depth) and dynamic covariates (e.g., SST or BT). Additionally, many times we are interested in including dynamic covariates across different spatial or temporal scales. For example, we may want a seasonal average SST AND an annual minimum SST at each trawl location. What this all means is that this isn't a simple extraction -- we can't just read in an existing depth raster and then stack on a seasonal SST layer. For one thing, this "seasonal" SST layer doesn't exist, and even if it did, what about if we wanted to use some other temporal scale? So, we either have to create these layers ourselves or we need different functions depending on if we are extracting static or dynamic covariates at point locations. I *think* the easiest approach is different functions? The other important thing to note is that any work with the dynamic covariates first involves gathering those data. A good example of this is the NOAA OISST product. This is available through any number of THREDDS or ERDDAP servers and is commonly stored as yearly (or daily) files. To get a full spatio-temporal time series of the data, we have to create it. There's a slew of different approaches for doing this -- ODP has theirs, and I think Adam, Matt and I all have our own process, too. It would be great if this was something that was automated and all of us knew right where the most recent, collated global OISST data were. With that dream goal in mind, going to think about a static extraction function and a dynamic extraction function that each use raster data and spatial points as input.

#### Static covariate extraction
Within the "collecting covariates" workflow step, I'll focus on the static covariates first. This is a really straightforward process and just amounts to overlaying our data, represented as spatial points, onto a raster stack, with each layer in the stack representing a static variable of interest (e.g., depth, habitat type). I'm trying to think of how this function might be expanded and the only other thing I can think of is if we wanted to collect information over different spatial scales than the input raster stack **which have to be of the same spatial extent and scale**. Something to think about for potential function improvements down the road.  

**Input**  
    +  rast_stack = A raster stack of covariates.
    +  sf_points = A sf spatial points object with the locations where we want to extract covariate values.  

**Output**  
    +  A tidy data set, where each row is a unique tow-species observation with additional columns for each of the covariate values.  

**Other Arguments**  
    +  stack_names = A vector of the names of each of these covariates.  
    +  out_path = Path to where we want to save the new file, which includes the prepped trawl data with appended columns for each of the covariates.  
    +  new_file_name = Name for new file. Default is NULL, which will override the `model_dat.rds` file saved during the processing step.

```{r, eval = eval_use}
# Which static covariate values do we want to include? Provide each of them as a layer in a raster stack
cov_stack<- raster::stack(paste(shared.path(os.use = os_use, group = "root", folder = "RES Data/Shapefiles/"), "NEShelf_Etopo1_bathy.tiff", sep = ""))

# Convert trawl data to sf spatial points. I've thought of different approaches to this and I think in the end it might make the most sense to have one "trawl" data set for these extractions, which is just the location of each unique tow. This would reduce the extraction time (not running it for each species obs at each tow). 
trawl_dat<- nefsc_dat %>%
  ungroup() %>%
  distinct(., EST_DATE, DECDEG_BEGLON, DECDEG_BEGLAT)
trawl_sf<- trawl_dat %>%
  st_as_sf(., coords = c("DECDEG_BEGLON", "DECDEG_BEGLAT"), crs = 4326, remove = FALSE)

# Run static extraction function and return it as sf object as we will want to extract some more covariates and need things as sf
trawl_covs<- static_extract(rast_stack = cov_stack, stack_names = "DEPTH", sf_points = trawl_sf, out_path = here::here("/scratch/aja/data/"), df_sf = "sf")
```

#### Dynamic covariate extraction
Next are the dynamic covariates. As discussed above, these are a bit trickier because of the different temporal (and potentially spatial) scales folks might be interested in summarizing covariate values over, and also because we generally have to compile these on our own. For now, I'm going to again pretend like these dynamic covariate data were updated regularly and stored somewhere easily accessible for all functions to work from. With this extraction function, I am sure there are improvements that we can make! The trick will be making sure the function is still relatively understandable and simple to execute, while also being flexible enough to accommodate different situations that is actually useful. 

**Input**  
    +  rast_ts_stack = A raster stack of one covariate variable, where each layer represents a different time step.  
    +  t_summ = A numeric value or character string that indicates what temporal resolution should be used in summarizing the covariate values. If numeric, the function will simply summarize the values in the raster stack from the matching period back `t_summ` numeric time steps. For example, if the `rast_ts_stack` provided daily values and t_summ = 90, the function would calculate a 90 day average, where the 90-day window would either be leading up to and including the day of observation, saddled around the observation, or include the day of the observation and 89 days into the future. If a character string, should be one of "daily", monthly", "seasonal", or "annual". These options are built into the function to provide a bit easier specification to quickly calculate the monthly/seasonal/annual summaries of the raster stack values. When used, this automatically defines `t_position = saddle`.
    +  t_position = A character vector of either NULL, "past", "saddle", or "future". If NULL, then values are extracted based on matching up the observation point with the dynamic raster stack at the level specified in the t_summ character vector (e.g., "daily", "monthly", "seasonal", "annual". If not, then summaries are calculated leading up to and including the observation time ("past"), saddled around around the observation time ("saddle"), or including and in the future of the observation time ("future").
    +  sf_points = A sf spatial points object with the locations where we want to extract covariate values AND the date of the observations.  

**Output**  
    +  A tidy data set, where each row is a unique tow-species observation and that has an appended column with the dynamic covariate value.  

**Other Arguments**  
    +  stack_name = A character string specifying the name for the dynamic covariate column. 
    +  df_sf = Character string one of "df" or "sf" signaling whether the returned object should be a data frame or sf object 
    +  out_path = Path to where we want to save the new file, which includes the prepped trawl data with appended columns for the new covariate. 
    +  new_file_name = Name for new file. Default is NULL, which will override the `model_dat.rds` file saved during the processing step.

```{r, eval = eval_use}
# OISST stack
oisst_stack<- raster::stack(paste(shared.path(os.use = os_use, group = "root", folder = "RES_Data/OISST/"), "ThroughFeb2020.grd", sep = ""))
# Annoyingly, dates not preserved...
oisst_dates<- seq(from = ymd('1981-09-01'), to = ymd('1981-09-01') + nlayers(oisst_stack)-1, by = 'day')
names(oisst_stack)<- oisst_dates

# Run function, getting seasonal mean temps at each tow location
trawl_covs<- dynamic_2d_extract(rast_ts_stack = oisst_stack, stack_name = "SST", t_summ = "seasonal", t_position = NULL, sf_points = trawl_sf, out_path = here::here("/scratch/aja/data/"), df_sf = "sf", new_file_name = NULL)
```

#### Covariate extraction wrap-up
After extracting the covariate values, we can then take a look at some quick summaries to make sure everything "worked." 
```{r, eval = eval_use}
# Take a look at the covariate values
summary(trawl_covs)

# There are some NAs with the SST_seasonal covariate. Do these make sense given dates of the tows and the dates of available SST data (1981 ish-present)?
trawl_sst_nas<- trawl_covs %>%
  filter(., is.na(SST_seasonal))
summary(trawl_sst_nas)

# Some, but not all. What about the ones that are within the data range...
trawl_sst_check<- trawl_sst_nas %>%
  filter(., EST_DATE >= as.Date("1982-01-01"))

# Where are these?
os.use<- "unix"
res_dat_path<- shared.path(os.use = os_use, group = "root", folder = "RES_Data")
land<- st_read(paste(res_dat_path, "Shapefiles/ne_50m_land/ne_50m_land.shp", sep = "")) 

# Visualize
xlim<- c(-76, -65) 
ylim<- c(35, 45)

na_plot<- ggplot() +
  geom_sf(data = land, fill = "#f6f6f6", color = "light gray") +
  geom_sf(data = trawl_sst_check) +
  coord_sf(xlim, ylim, expand = FALSE) +    
  xlab("") +
  ylab("") +
  theme_bw()
na_plot

# These points are certainly "coastal", but doesn't seem to make that much sense given the footprint of the OISST data.
sst_foot<- as.data.frame(oisst_stack[[1]], xy = TRUE)
names(sst_foot)[3]<- "sst"

na_sst_plot<- ggplot() +
  geom_tile(data = sst_foot, aes(x = x, y = y, fill = sst)) +
  scale_fill_viridis_c() +
  geom_sf(data = land, fill = "#f6f6f6", color = "light gray") +
  geom_sf(data = trawl_sst_check) +
  coord_sf(xlim, ylim, expand = FALSE) +    
  xlab("") +
  ylab("") +
  theme_bw()
na_sst_plot

# Welp, looks like that is actually what is going on. One option would be to just interpolate SST at these locations given SST at neighboring points on the same day/season.
time_match<- trawl_covs %>%
  filter(., Season_Match %in% trawl_sst_check$Season_Match)

for(i in seq_along(trawl_sst_check$Season_Match)){
  time_match_temp<- time_match %>%
    filter(., Season_Match == trawl_sst_check$Season_Match[i]) %>%
    # Drop NAs -- other wise, we will wind up keeping our exact point
    filter(., !is.na(SST_seasonal))
  nearest_out<- st_nearest_feature(trawl_sst_check[i,], time_match_temp)
  trawl_sst_check$SST_seasonal[i]<- time_match_temp$SST_seasonal[nearest_out]
}

# Okay, back together with the "non-NA"s...
trawl_sst_comp<- trawl_covs %>%
  filter(., !is.na(SST_seasonal)) %>%
  rbind(., trawl_sst_check)
summary(trawl_sst_comp)
```

So far, we have only covered the static and dynamic covariates that have one "level", where we can simply overlay the points on the raster layers to get the values. There are some examples where we will actually be working with dynamic covariates that have multiple "levels." A great example is bottom temperature and the Simple Ocean Data Assimilation product. With this product, there are 50 different "levels" or modeled depth layers. At any given location, while there are going to be 50 different potential levels, there may only be modeled temperatures for a subset of those levels depending on how deep the model domain thinks the ocean is at that location. For instance, a shallow spot might only have a temperature at the "surface" (level = 1) and the second subsequent model level (level = 2). We will likely want to have a function that can handle this -- maybe building from the `dynamic_extract` function with an added "level" argument, where the user could provide a vector of levels to extract. 

### Fitting a "basic" species distribution model
#### Setting up model data set
First things first, need to bring the covariates over with the full observation data set as for the extraction components we were only with unique tows (a combination of `EST_DATE`, `DECDEG_BEGLON` and `DECDEG_BEGLAT`).
```{r, eval = eval_use}
# Covariate data
summary(trawl_sst_comp)

# Observation data
summary(nefsc_dat)

# Convert to data frame and then join based on EST_DATE, DECDEG_BEBLON, DECDEG_BEGLAT
str(nefsc_dat)
str(trawl_sst_comp)

trawl_sst_comp<- trawl_sst_comp %>%
  st_drop_geometry()

mod_dat<- nefsc_dat %>%
  ungroup() %>%
  left_join(., trawl_sst_comp, by = c("EST_DATE" = "EST_DATE", "DECDEG_BEGLON" = "DECDEG_BEGLON", "DECDEG_BEGLAT" = "DECDEG_BEGLAT"))
```

#### Model fitting
Things get a bit complicated during this step given the number of different species in the data set and then the number of different approaches that folks may want to try. So, rather than spending a whole lot of time thinking about a function (or functions) for this section, I think the best path forward might instead be thinking about the general guidelines for any function (or functions) that could be plugged into this step in the workflow. To start, we can think about the basic of what the input to a given function would be and then what we would want to have as an output. Focusing on these pieces, I think would then allow folks to explore their own model approaches, while ensuring that the function integrates with the overall workflow.

**Model fitting input** 
At a bare minimum, any function within this section should be able to take in a spatio-temporal data set, where each row is an observation describing the occurrence of a species at a given location at a given time. I *think* the function will also want some flexibility, where the user can specify the model formula as an argument to the fitting function rather than having a model formula hard-wired into the fitting function.

**Model fitting output**
We will likely have similar issues with the model fitting output as the model fitting input step. I am not really sure what the best way forward is? Certainly, there are common fitted model objects that are regularly handled by existing functions. For example, `predict` can handle generalized linear models, generalized additive models, random forests, etc. For others though, specifically the VAST modeling framework, there is not a pre-built `predict` style function. For now, I guess I would think that the requirements for the output would be that the returned model fit object either needs to be easily incorporated into existing functions OR come with associated functions that support making statistical inferences from the fitted models, including extracting parameter estimates/uncertainty, hypothesis testing, model selection and model prediction. 

**Model fitting example function -- seasonal two-stage delta log normal generalized additive model**
To keep things simple, just going to use the species distribution modeling approach that we used in the COCA I project. The two-stage delta generalized additive model (GAM) has been widely used in other marine fish distribution modeling studies and has several advantages. First, the two stage approach models presence/absence and then models the log positive biomass observations, and this structure accommodates situations where the number of absence observations exceeds those expected from traditional “count” distributions (e.g., Poisson, tweedie). Second, the additive modeling framework requires no a priori assumptions about the functional relationships between the response (species presence/absence and biomass) and predictor variables, allowing for non-linear relationships. For each species, we will fit seasonally-independent models (e.g., a spring and a fall model). 

Before jumping into fitting the model for a given species-season data set, there's the remaining question of **how to efficiently apply the same model across multiple species-seasons?** In the COCA I project, I ended up doing this leveraging the `map` style functions of the `purrr` library. I am curious to hear what others think about this process. Personally, I think it makes things rather easy to "see" and then it also sets up a nice workflow where subsequent inference functions (e.g., parameter estimates, hypothesis testing, model selection and model prediction) can also be implemented in a similar way. This results in an object that has multiple rows, one for each season-species modeled, and then columns for the species-season's data, the fitted model object, model predictions, etc. 

Finally, there's one other part of this that might look a bit different than the typical SDM workflow. This difference arises from our interest in validating the predictive skill of the distribution models that are fit to scaled/centered variables. On the predictive skill side, we set up training and testing data sets, which is pretty common when fitting models for forecasting/projecting purposes. For the scaled/centered bit, we basically want to make sure that any "new data" used to make predictions/forecasts/projections is "rescaled" appropriately. Many SDM approaches automatically scale/center or standardize the continuous predictor variables to help the model converge and to put variables on somewhat equal footing. *I think GAM does this?* When we take the next step to use the model to predict/forecast/project, we are implicitly assuming that the "scaling" of the new data matches the "scaling" of the data used in the model fitting. In most cases, this might not matter -- especially if the scaling/centering is done BEFORE the splitting into training/testing data. For our interest in species distribution projections using temperatures from global climate models, this seems problematic. In particular, those projected future temperatures are going to likely be outside the range of variability in temperatures during the observation period. So, just to be completely transparent, I go through the process of centering/scaling the two covariates and then "rescaling" any new data values according to the mean/sd of the variable in the fitted data. 

*Creating nested dataframe, with training/testing subsets AND retaining variable scaling information to make sure we "rescale" variable values when making predictions to different time periods/regions*
```{r, eval = eval_use}
# For COCA I, the first thing I did was set up training and testing data sets for each species-season.
# Training vs. testing
train.start<- "1982-01-01"
train.end<- "2012-12-31"
test.start<- "2013-01-01"
test.end<- "2019-01-01"

mod_dat$Train_Test<- ifelse(mod_dat$EST_DATE >= train.start & mod_dat$EST_DATE <= train.end, "Train", 
                        ifelse(mod_dat$EST_DATE >= test.start & mod_dat$EST_DATE <= test.end, "Test", "Neither"))

# Training data by season: fall and then spring, scaling variables before fitting models and defining "presence" response for presence model component
train_dat_f<- mod_dat %>%
  filter(., Train_Test == "Train" & SEASON == "FALL") %>%
  mutate(., "Depth_scaled" = as.numeric(scale(AVGDEPTH)),
         "SST_seasonal_scaled" = as.numeric(scale(SST_seasonal))) 
train_dat_s<- mod_dat %>%
  filter(., Train_Test == "Train" & SEASON == "SPRING") %>%
  mutate(., "Depth_scaled" = as.numeric(scale(AVGDEPTH)),
         "SST_seasonal_scaled" = as.numeric(scale(SST_seasonal))) 


# Need to retain the scaling information used for each of these to then correctly rescale both the testing SST_seasonal values, and more importantly, temperatures from the global climate models. 
fall_rescale<- train_dat_f %>%
  group_by(., SEASON) %>%
  summarize_at(., .vars = c("AVGDEPTH", "SST_seasonal"), .funs = c("Mean" = mean, "SD" = sd))

spring_rescale<- train_dat_s %>%
  group_by(., SEASON) %>%
  summarize_at(., .vars = c("AVGDEPTH", "SST_seasonal"), .funs = c("Mean" = mean, "SD" = sd))

# Testing data frames, and rescaling covariates using appropriate mean and sd.
# First, a rescaling function
cov_scale<- function(raw_val, scale_mean, scale_sd){
  # Deal with NA values
  if(is.na(raw_val)){
    cov_scale_out<- NA
    return(cov_scale_out)
  } else {
    # Scale using mean and sd
    cov_scale_out<- (raw_val - scale_mean)/scale_sd
    return(cov_scale_out)
  }
}

# Fall
test_dat_f<- mod_dat %>%
  filter(., Train_Test == "Test" & SEASON == "FALL") %>%
  mutate(., "Depth_scaled" = pmap_dbl(list(raw_val = AVGDEPTH, scale_mean = list(fall_rescale$AVGDEPTH_Mean), scale_sd = list(fall_rescale$AVGDEPTH_SD)), cov_scale),
         "SST_seasonal_scaled" = pmap_dbl(list(raw_val = SST_seasonal, scale_mean = list(fall_rescale$SST_seasonal_Mean), scale_sd = list(fall_rescale$SST_seasonal_SD)), cov_scale))

# Spring
test_dat_s<- mod_dat %>%
  filter(., Train_Test == "Test" & SEASON == "SPRING") %>%
  mutate(., "Depth_scaled" = pmap_dbl(list(raw_val = AVGDEPTH, scale_mean = list(spring_rescale$AVGDEPTH_Mean), scale_sd = list(spring_rescale$AVGDEPTH_SD)), cov_scale),
         "SST_seasonal_scaled" = pmap_dbl(list(raw_val = SST_seasonal, scale_mean = list(spring_rescale$SST_seasonal_Mean), scale_sd = list(spring_rescale$SST_seasonal_SD)), cov_scale))


# Alright, now nested data frames and then the training/testing data sets as different columns...
# Training
all_train<- train_dat_f %>%
  bind_rows(., train_dat_s) %>%
  group_nest(., COMNAME, SEASON, .key = "Training_data") %>%
  arrange(COMNAME)

# Testing
all_test<- test_dat_f %>%
  bind_rows(., test_dat_s) %>%
  group_nest(., COMNAME, SEASON, .key = "Testing_data") %>%
  arrange(COMNAME) 

# Join
mod_dat<- all_train %>%
  left_join(., all_test, by = c("COMNAME", "SEASON"))

# This results in a LOT of unique data sets to model (956 to be exact). Rather than dealing with all of those, let's reduce it down a bit and just look at two: Atlantic cod and Haddock
spp_keep<- c("ATLANTIC COD", "HADDOCK")
mod_dat_use<- mod_dat %>%
  filter(., COMNAME %in% spp_keep)
```

*Two-stage delta log normal GAM fitting function*
```{r, eval = eval_use}
# A function to fit the two stage GAM. With these two stage models, we need to fit and save both the presence/absence first stage of the model AND the second log positive biomass stage of the model. 

gam_fit<- function(df, mod_formula, response){
  if(FALSE){
    df = mod_dat_use$data[[1]]
    mod_formula = "PRESENCE ~ s(Depth_scaled, fx = FALSE, bs = 'cs') + s(SST_seasonal_scaled, fx = FALSE, bs = 'cs')"
    response = "Presence"
  }
  
  formula_use<- as.formula(mod_formula)
  if(response == "Presence"){
    gam_out<- gam(formula_use, drop.unused.levels = T, data = df, family = binomial(link = logit), select = TRUE)
    return(gam_out)
  } 
  
  if(response == "Biomass"){
    gam_out<- gam(formula_use, drop.unused.levels = T, data = df, family = gaussian, select = TRUE)
    return(gam_out)
  }
}

# Now, map this function to the different species-season data sets. Do this using "possibly", so that the function is tried and returns NA if for some reason the function returns an error (for example, because there aren't adequate data to fit the model)
mod_res<- mod_dat_use %>%
    mutate(., "mod_fitted_p" = pmap(list(df = Training_data, mod_formula = list("PRESENCE ~ s(Depth_scaled, fx = FALSE, bs = 'cs') + s(SST_seasonal_scaled, fx = FALSE, bs = 'cs')"), response = list("Presence")), possibly(gam_fit, NA)),
           "mod_fitted_b" = pmap(list(df = Training_data, mod_formula = list("LOG_BIOMASS ~ s(Depth_scaled, fx = FALSE, bs = 'cs') + s(SST_seasonal_scaled, fx = FALSE, bs = 'cs')"), response = list("Biomass")), possibly(gam_fit, NA)))

# Check that things "worked" -- should have a GAM model object for each of the species-seasons and model stage columns
mod_res

# Look at one
summary(mod_res$mod_fitted_b[[3]])
```

### Statistical Inference from fitted model
#### Parameter estimation, hypothesis testing, model evaluation
With the fitted models, the next thing we are usually interested in doing is making some statistical inferences from the fitted models. This could be something like looking at the fitted smooth functions for each covariate (or point parameter estimates for a linear model), or performing some hypothesis test or model selection procedure. The nice thing about having the models fit to the nested data frames is that we can really easily map statistical inference functions to the fitted model object columns. Many of these are already built in to the `broom` library. For example, we may use `tidy` to extract the smooth function (or parameter estimates) component of the species distribution model, `augment` to get the model fitted and residual values for each of the observations, or `glance` to calculate statistics to evaluate the model fit to the data. 
```{r, eval = eval_use}
# Using functions in the broom library to make statistical inferences with the fitted model
mod_res<- mod_res %>%
  mutate(., "Params_p" = map(mod_fitted_p, possibly(tidy, NA)),
         "Params_b" = map(mod_fitted_b, possibly(tidy, NA)),
         "Eval_p" = map(mod_fitted_p, possibly(glance, NA)),
         "Eval_b" = map(mod_fitted_b, possibly(glance, NA)))

mod_res$Params_b[[3]]
mod_res$Eval_b[[3]]
```

#### Model predictions: Validating predictive skill
**Predicting to testing data**
In this project, we are mostly interested in using the fitted model to make predictions. For the data used IN the model fitting process, we might be able to leverage `augment` to get these predictions for some fitted model objects. However, the best I can tell, there is not an `augment` method implemented for `mgcv::gam` objects. This is probably for the best though as it means we need to write our own `predict` style function, which will give us some more flexibility and understanding of what is going on. 

To make these predictions, I am going to use a simple, yet somewhat flexible, prediction function (`sdm_predict`) that we can then map to each row of our tidy `mod_res` data set. 

**Input**  
    +  mod_fit_p = The fitted model object for the presence/absence stage 
    +  mod_fit_b = The fitted model object for the log(biomass) stage
    +  response = A character vector of either "Presence" or "Biomass", which will indicate whether the function should return either the predicted probability of presence (ignores mod_fit_b) or the relative biomass (multiplies predictions from mod_fit_p and mod_fit_b)
    + test_dat = A data frame that represents the locations (space and time), which we want to make predictions. This prediction data frame HAS to include values for all of the covariates used in the fitted model.  
    
**Output**  
    + A data frame with the location (long, lat, and time) and model predicted value

```{r, eval = eval_use}
# Create the prediction function
predict_func<- function(mod_fit_p, mod_fit_b, response, test_dat) {
  if(FALSE){
    mod_fit_p = mod_res$mod_fitted_p[[1]]
    mod_fit_b = mod_res$mod_fitted_b[[1]]
    response = "Biomass"
    test_dat = mod_res$data[[1]]
  }
  
  # Get location information and make predictions
  pred_df<- test_dat %>%
    select(., EST_DATE, DECDEG_BEGLON, DECDEG_BEGLAT, SUM_BIOMASS)
  
  pred_p<- round(as.numeric(predict.gam(mod_fit_p, newdata = test_dat, type = "response", se.fit = TRUE)$fit), 3)
  
  if(response == "Biomass"){
    # Need biomass prediction and then multiple predictions from both stages
    pred_logb<- exp(round(as.numeric(predict.gam(mod_fit_b, newdata = test_dat, type = "response", se.fit = TRUE)$fit), 3))
    pred_b<- pred_p*pred_logb
    return(data.frame(pred_df, "Pred" = pred_b))
  } else {
    # Response is presence, so just return that
    return(data.frame(pred_df, "Pred", pred_p))
  }
}

# Map it to each of the season-species model stages. Here, just going to keep the relative biomass predictions.
mod_res<- mod_res %>%
  mutate(., "Pred_Baseline_Biomass" = pmap(list(mod_fit_p = mod_fitted_p, mod_fit_b = mod_fitted_b, response = "Biomass", test_dat = Testing_data), possibly(predict_func, NA)))

mod_res
summary(mod_res$Pred_Baseline_Biomass[[1]])
```

**Comparing predictions to observations**
Now that we have made the predictions to the testing data, we can validate the predictive skill of the model by comparing these predictions to the observations. There are a mess of different statistics that you could calculate to assess this predictive skill. Personally, I really like using Taylor Diagrams for this purposes as it allows us to visually assess the predictive skill across multiple components: the relative correlation between model predictions and observations, the absolute error between model predictions and observations, and the aggregated spatial bias between model predictions and observations. 
```{r, eval = eval_use}
mod_res<- mod_res %>%
  mutate(., "TaylorDiagram" = pmap(list(dat = Pred_Baseline_Biomass, obs = "SUM_BIOMASS", mod = "Pred", pt_col = list("#006d2c")), possibly(taylor_diagram, NA)))
mod_res$TaylorDiagram[[1]]
```

### Model projections: bringing in climate data
After fitting the model and then validating its predictive skill against a set of testing data, we are ready to take the next step and use the model to project changes in distribution and abundance based on potential future environmental conditions. During the COCA I project, we used future environmental conditions from RCP4.5 and RCP8.5 scenario experiments, focusing on mid-century (2055) and end-of-century (2100) time horizon snapshots and using the 5th percentile, mean, and 95th percentile of the climate model ensemble for each scenario experiment. So, for a given species-season and response (presence/absence or biomass), we were making 12 projections (2 scenario experiments * 2 time horizons * 3 climate model ensemble statistics). This ended up getting really messy really fast and **I'd love some input from other folks on how we might do this more efficiently.** When I did it, it amounted to a whole lot of `mutate` -- mostly because in the beginning I was really only working with three projections (RCP8.5, 2055, 5th/mean/95th) and then we added in the RCP4.5 and different time horizons later on.

In any event, the process for making these projections is identical to the one used above to make the predictions, with a few key differences. First, there's a whole lot of work that needs to be done to gather the climate model projections from online sources, process them to a common grid, calculate anomalies and then bias correct these. Second, rather than projecting to specific points as we did when predicting to the hold out testing data, we are going to be making projections for all grid cells within a specified spatial domain. Third, for a given projection we may want to summarize and visualize the differences on either the raw measurement scale or as a percentage. 

Ideally, we could work together to generate a robust function for this. For now, though, just going to follow along with the script I used during COCA I to accomplish this goal. At it's core, this script relies on two data sets that have the information needed to project species distribution and abundance, which is stored as a flat file with location information and then columns for depth, as well as the different projected temperatures. For example, there is a column "Spring.2025.rcp85.mu", which is the average spring temperature value in 2025 across the climate model member ensembles run for the RCP8.5 scenario. Like a lot of the COCA I project, as we started to add scenarios/time horizons, things became more and more "messy" and I put more attention on producing the results than "optimizing" the code. **I am sure there is a better way of doing all of this!!* 
```{r, eval = eval_use}
# Read in "future" projection data, which are stored as RData files. The code to generate these files is admittedly a bit messy, but it can be seen here: https://github.com/aallyn/COCA-SDM/blob/master/Code/sdm_projectionvariable_rasters.R. Again, looking back on things I feel 
spring_proj_dat<- readRDS(here::here("scratch/aja/Data", "spring.rast.preds.01122021.rds"))
fall_proj_dat<- readRDS(here("scratch/aja/Data", "fall.rast.preds.01122021.rds"))

# To keep things simple, lets just use the Baseline and the 2055 rcp85 and rcp45 mean scenario
spring_proj_sub<- spring_proj_dat %>%
  select(., x, y, SEASON, DEPTH, Baseline, Spring.2055.rcp45.mu, Spring.2055.rcp85.mu)
fall_proj_sub<- fall_proj_dat %>%
  select(., x, y, SEASON, DEPTH, Baseline, Fall.2055.rcp45.mu, Fall.2055.rcp85.mu)

# Now, we need to rescale these values...
spring_proj_use<- spring_proj_sub %>%
 mutate(., "Depth_scaled" = pmap_dbl(list(raw_val = abs(DEPTH), scale_mean = list(spring_rescale$AVGDEPTH_Mean), scale_sd = list(spring_rescale$AVGDEPTH_SD)), cov_scale),
         "Baseline_sst_scaled" = pmap_dbl(list(raw_val = Baseline, scale_mean = list(spring_rescale$SST_seasonal_Mean), scale_sd = list(spring_rescale$SST_seasonal_SD)), cov_scale),
        "RCP45_2055_mu_scaled" = pmap_dbl(list(raw_val = Spring.2055.rcp45.mu, scale_mean = list(spring_rescale$SST_seasonal_Mean), scale_sd = list(spring_rescale$SST_seasonal_SD)), cov_scale),
        "RCP85_2055_mu_scaled" = pmap_dbl(list(raw_val = Spring.2055.rcp85.mu, scale_mean = list(spring_rescale$SST_seasonal_Mean), scale_sd = list(spring_rescale$SST_seasonal_SD)), cov_scale)) %>%
  select(., -Spring.2055.rcp45.mu, -Spring.2055.rcp85.mu)

fall_proj_use<- fall_proj_sub %>%
 mutate(., "Depth_scaled" = pmap_dbl(list(raw_val = abs(DEPTH), scale_mean = list(fall_rescale$AVGDEPTH_Mean), scale_sd = list(fall_rescale$AVGDEPTH_SD)), cov_scale),
         "Baseline_sst_scaled" = pmap_dbl(list(raw_val = Baseline, scale_mean = list(fall_rescale$SST_seasonal_Mean), scale_sd = list(fall_rescale$SST_seasonal_SD)), cov_scale),
        "RCP45_2055_mu_scaled" = pmap_dbl(list(raw_val = Fall.2055.rcp45.mu, scale_mean = list(fall_rescale$SST_seasonal_Mean), scale_sd = list(fall_rescale$SST_seasonal_SD)), cov_scale),
        "RCP85_2055_mu_scaled" = pmap_dbl(list(raw_val = Fall.2055.rcp85.mu, scale_mean = list(fall_rescale$SST_seasonal_Mean), scale_sd = list(fall_rescale$SST_seasonal_SD)), cov_scale)) %>%
  select(., -Fall.2055.rcp45.mu, -Fall.2055.rcp85.mu)

# Nest and then combine into one "proj_dat" data frame
proj_dat<- spring_proj_use %>%
  bind_rows(., fall_proj_use) %>%
  group_nest(., SEASON, .key = "Projection_data")
```

Now, creating and mapping a projection function, very similar to the prediction process earlier.
```{r, eval = eval_use}
# Probably a better way to do this, too. For now, just going to bring over the proj_dat into the mod_res data frame.
mod_res<- mod_res %>%
  left_join(., proj_dat, by = c("SEASON" = "SEASON"))

# Create the projection function
proj_func<- function(mod_fit_p, mod_fit_b, response, proj_dat, proj_temp) {
  if(FALSE){
    mod_fit_p = mod_res$mod_fitted_p[[1]]
    mod_fit_b = mod_res$mod_fitted_b[[1]]
    response = "Biomass"
    proj_dat = mod_res$Projection_data[[1]]
    proj_temp = "Baseline_sst_scaled"
  }
  
  # Get location information and covariates
  pred_df<- proj_dat %>%
    select(., x, y, DEPTH, {{proj_temp}})
  
  # Address naming issues
  old_names<- names(pred_df)[3:4]
  new_names<- attr(mod_fit_p$terms,"term.labels")
  pred_df<- pred_df %>%
    rename_at(vars(all_of(old_names)), ~ new_names)
  
  # Make predictions
  pred_p<- round(as.numeric(predict.gam(mod_fit_p, newdata = pred_df, type = "response", se.fit = TRUE)$fit), 3)
  
  if(response == "Biomass"){
    # Need biomass prediction and then multiple predictions from both stages
    pred_logb<- exp(round(as.numeric(predict.gam(mod_fit_b, newdata = pred_df, type = "response", se.fit = TRUE)$fit), 3))
    pred_b<- pred_p*pred_logb
    return(data.frame(pred_df, "Pred" = pred_b))
  } else {
    # Response is presence, so just return that
    return(data.frame(pred_df, "Pred", pred_p))
  }
}

# Map it with mutate
mod_res<- mod_res %>%
  mutate(., "Proj_Baseline_Biomass" = pmap(list(mod_fit_p = mod_fitted_p, mod_fit_b = mod_fitted_b, response = "Biomass", proj_dat = Projection_data, proj_temp = "Baseline_sst_scaled"), possibly(proj_func, NA)),
         "RCP45_2055_Mu_Biomass" = pmap(list(mod_fit_p = mod_fitted_p, mod_fit_b = mod_fitted_b, response = "Biomass", proj_dat = Projection_data, proj_temp = "RCP45_2055_mu_scaled"), possibly(proj_func, NA)),
         "RCP85_2055_Mu_Biomass" = pmap(list(mod_fit_p = mod_fitted_p, mod_fit_b = mod_fitted_b, response = "Biomass", proj_dat = Projection_data, proj_temp = "RCP85_2055_mu_scaled"), possibly(proj_func, NA)))
```
### Summarizing results: Projected changes across spatial scales from the large marine ecosystem to coastal communities
After making the projections, we can calculate changes in relative biomass for the different climate scenarios/ensemble statistic/time horizons for each location within the Northeast U.S. Large Marine Ecosystem. In COCOA I, we then summarized these changes across a few different scales: the large marine ecosystem, biophysical regions within the large marine ecosystem (Gulf of Maine vs. southern New England/Mid-Atlantic Bight), and then finally, coastal communities using vessel trip reports. 

There's some weirdness that arises here, especially related to trying to calculate and visualize projected changes as percentages. If we try to do the calculation of the percentage at each cell and then average these within a spatial area of interest, we run into issues with `-Inf` when the baseline biomass at the cell is 0. To get around this, during the COCA I project, we first calculated the average biomass within the spatial area for each period (e.g., baseline biomass and then the climate scenarios/ensemble statistic/time horizons projected biomass). Then we perform the percentage calculations based on those average values. 

#### Summarizing differences across biophysical regions
First, let's use an overlay function that can take in different shapefiles and then calculate the average biomass (or probability of presence) within that area. For now, we can use a shapefile that includes the Northeast U.S. LME, the Gulf of Maine, and then just the southern New England/Mid-Atlantic Bight area.

```{r, eval = eval_use}
# Regions of interest
nelme_sf<- st_read(paste(res_dat_path, "/Shapefiles/NELME_regions/NELME_sf.shp", sep = ""))
gom_sf<- st_read(paste(res_dat_path, "/Shapefiles/NELME_regions/GoM_sf.shp", sep = ""))
snemab_sf<- st_read(paste(res_dat_path, "/Shapefiles/NELME_regions/SNEandMAB_sf.shp", sep = ""))

# Visualize them quickly. Wrong! This takes forever, so commenting out for now.
# spat_area_plot<- ggplot() +
#   geom_sf(data = nelme_sf, fill = NA, color = "#1b9e77", lwd = 2) +
#   geom_sf(data = gom_sf, fill = NA, color = "#7570b3", lwd = 1.25) +
#   geom_sf(data = snemab_sf, fill = NA, color = "#d95f02", lwd = 1.25) +
#   geom_sf(data = land, fill = "#f6f6f6", color = "light gray") +
#   coord_sf(xlim, ylim, expand = FALSE) +    
#   xlab("") +
#   ylab("") +
#   theme_bw()

# Now, the overlay func
shp_over_pts_func<- function(dat, shp_file){
  if(FALSE){
    dat<- mod_res$RCP45_2055_Mu_Biomass[[1]]
    shp_file<- nelme_sf
  }
  
  # Convert data frame to sf object
  dat_sf<- dat %>%
    st_as_sf(., coords = c("x", "y"), crs = 4326, remove = FALSE)
  
  # Calculate average with sf aggregate and then simplify things to just return the average value
  mean_out<- aggregate(dat_sf, shp_file, mean, na.rm = TRUE, join = st_intersects) %>%
    st_drop_geometry() %>%
    select(., Pred) %>%
    as.numeric()
  return(mean_out)
}
```

Now, map that function to our `mod_res` object to get the average projected biomass under different conditions within the NELME, GoM and SNE-MAB regions. Then, make a quick plot showing an example of the results.
```{r, eval = eval_use}
# Mapping it to our mod_res dataframe. Name explosion -- help!!!
mod_res<- mod_res %>%
  mutate(., "NELME_Baseline_Biomass" = map2_dbl(Proj_Baseline_Biomass, list(nelme_sf), shp_over_pts_func),
         "NELME_RCP45_2055_Biomass" = map2_dbl(RCP45_2055_Mu_Biomass, list(nelme_sf), shp_over_pts_func),
         "NELME_RCP85_2055_Biomass" = map2_dbl(RCP85_2055_Mu_Biomass, list(nelme_sf), shp_over_pts_func),
         "GoM_Baseline_Biomass" = map2_dbl(Proj_Baseline_Biomass, list(gom_sf), shp_over_pts_func),
         "GoM_RCP45_2055_Biomass" = map2_dbl(RCP45_2055_Mu_Biomass, list(gom_sf), shp_over_pts_func),
         "GoM_RCP85_2055_Biomass" = map2_dbl(RCP85_2055_Mu_Biomass, list(gom_sf), shp_over_pts_func),
         "SNEMAB_Baseline_Biomass" = map2_dbl(Proj_Baseline_Biomass, list(snemab_sf), shp_over_pts_func),
         "SNEMAB_RCP45_2055_Biomass" = map2_dbl(RCP45_2055_Mu_Biomass, list(snemab_sf), shp_over_pts_func),
         "SNEMAB_RCP85_2055_Biomass" = map2_dbl(RCP85_2055_Mu_Biomass, list(snemab_sf), shp_over_pts_func))

# Percentage changes
cols_keep<- c("COMNAME", "SEASON", "NELME", "GoM", "SNEMAB")
proj_perc_change<- mod_res %>%
  select(., matches(paste(cols_keep, collapse="|"))) %>%
  mutate(., "NELME_RCP45_2055_PercChange" = 100*((NELME_RCP45_2055_Biomass - NELME_Baseline_Biomass)/NELME_Baseline_Biomass),
         "NELME_RCP85_2055_PercChange" = 100*((NELME_RCP85_2055_Biomass - NELME_Baseline_Biomass)/NELME_Baseline_Biomass),
         "GoM_RCP45_2055_PercChange" = 100*((GoM_RCP45_2055_Biomass - GoM_Baseline_Biomass)/GoM_Baseline_Biomass),
         "GoM_RCP85_2055_PercChange" = 100*((GoM_RCP85_2055_Biomass - GoM_Baseline_Biomass)/GoM_Baseline_Biomass),
         "SNEMAB_RCP45_2055_PercChange" = 100*((SNEMAB_RCP45_2055_Biomass - SNEMAB_Baseline_Biomass)/SNEMAB_Baseline_Biomass),
         "SNEMAB_RCP85_2055_PercChange" = 100*((SNEMAB_RCP85_2055_Biomass - SNEMAB_Baseline_Biomass)/SNEMAB_Baseline_Biomass))

# Quick plot...
# First get to a long format, filter to only the projected changes, and then create a column that has "region"
proj_perc_long<- proj_perc_change %>%
  pivot_longer(., cols = NELME_Baseline_Biomass:SNEMAB_RCP85_2055_PercChange, names_to = "Variable", values_to = "Value") %>%
  filter(., grepl("PercChange", Variable)) %>%
  separate(., Variable, c("Region", "Variable"), extra = "merge", remove = TRUE)
proj_perc_long$Region<- factor(proj_perc_long$Region, levels = c("NELME", "GoM", "SNEMAB"))

perc_change_plot<- ggplot(data = proj_perc_long, aes(x = COMNAME, y = Value, fill = Region)) + 
    geom_bar(stat = "identity", width = 0.6, position = position_dodge(width = 0.6)) +
    scale_fill_manual(name = "Region", values  = c("#1b9e77", "#7570b3", "#d95f02")) +
    ylab("Percent change in relative biomass") + 
    xlab("Species") +
    geom_hline(yintercept = 0) +
    theme_bw() +
    theme(text = element_text(size = 12),
          strip.background = element_blank(),
          panel.border = element_rect(colour = "black")) +
    coord_flip() +
    facet_wrap(~SEASON+Variable, scales = "free_y") +
    guides(fill = guide_legend(reverse = TRUE))
ggsave(here::here("scratch/aja/temp_results", "perc_change_example.png"))
```
```{r, eval = TRUE, out.width='75%', fig.align='center', fig.cap='Figure: Example of average percent change across the Northeast U.S. Large Marine Ecosystem, Gulf of Maine and Southern New England/Mid-Atlantic Bight'}
knitr::include_graphics(here::here("scratch/aja/temp_results", "perc_change_example.png"))
```

#### Summarizing differences at coastal communities
A key component to the COCA I project (and a likely reason why it was funded), was that we proposed to "localize" these regional changes down to coastal communities. To do this, we used vessel trip reports to generate "fishing footprints" for coastal communities by gear type. These fishing footprints represent locations where a vessel reported their location and catch as required by federal law. The vessel then could have gone on to fish/record other locations, eventually ending with a record of where the catch was off-loaded. Using the combination of the fished locations and the port of off-load, we created the fishing footprints. 

**Vessel Trip Reports (VTR) Fishing Footprints**
I am going to pick up this process with the footprints already processed. The creation of these footprints was done by someone else previously in the lab and the associated code can be seen at "Box/Mills Lab/Projects/COCA15_ClimVuln/Justin/". Of particular interest would be the "fishing footprint by year.R" code. Also, this section is going to be a bit more "code-y" than the other sections where I tried to make some attempt to use functions. This is for a couple of reasons, but the biggest is that it seems like the code/script style will be easier for people to follow.
```{r, eval = eval_use}
#####
## Reading in footprints and some naming clean up
#####
# Bring in fishing footprints. These are hosted on shared folder, unfortunately. 
foots_path<- "/Volumes/Shared/Research/COCA-conf/VTR fishing footprints by community and gear type 2011-2015.rds"
all_foot_dat<- readRDS(foots_path)
comm_names<- all_foot_dat$JGS.COMMUNITY
names(all_foot_dat)

# Going to work with the NOAA safe footprints, and regardless of which, want to add in a community name - cost ID name, where the cost ID is going to eventually be related to gear types
foot_safe<- TRUE
if(foot_safe){
  foot_dat<- all_foot_dat$JGS.NOAA.SAFE.COMMUNITY.GEAR.FOOTPRINTS
  names(foot_dat)<- paste(comm_names, all_foot_dat$COST_ID, sep = "_")
} else {
  foot_dat<- all_foot_dat$JGS.COMMUNITY.GEAR.FOOTPRINTS
  names(foot_dat)<- paste(comm_names, all_foot_dat$COST_ID, sep = "_")
}

# Want to add in gear type to the "name"
gear_types<- all_foot_dat$COST_ID

# Renaming -- better way to do this now I'm sure with tidyversing
gear_types<- ifelse(gear_types == 1, "Dredge",
                    ifelse(gear_types == 2, "Gillnet",
                           ifelse(gear_types == 3, "Longline",
                                  ifelse(gear_types == 4, "Pot/Trap",
                                         ifelse(gear_types == 5, "Purse/Seine",
                                                ifelse(gear_types == 6, "Trawl", "Other"))))))
comm_foot_names<- paste(comm_names, gear_types, sep = "-")
names(foot_dat)<- comm_foot_names

#####
## Pulling out the information we want for each of the footprints
#####
# The foot_dat object is a somewhat complicated list of lists. Check out the str of one of these to see (str(foot_dat[[476]])). Within that list, we can either look at the amount of kept catch at each grid cell (the second element of each list) or the proportion of catch kept at each grid cell relative to the total (the third element of each list). Going to go forward with that third component...

# Get proportion layer we want for each community-gear type. 
foots_rast<- unlist(lapply(foot_dat, "[", 3))
foots_stack<- raster::stack(foots_rast) # 476 layers

#####
## Creating some additional footprints: an all gear one 
#####
###
# All gear types
# For every community, we also want to have an "all gear" option, which would represent all unique cells fished across gears by vessels that landed catch at a specific community.
names_base<- names(foots_stack)
rast_ind<- nlayers(foots_stack)

# More naming nightmares..
if(foot_safe){
  comms_only<- str_replace_all(names(foots_stack), c(".Pot.Trap.JGS.SAFE.PROPORTION" = "", ".Other.JGS.SAFE.PROPORTION" = "", ".Gillnet.JGS.SAFE.PROPORTION" = "", ".Trawl.JGS.SAFE.PROPORTION" = "", ".Dredge.JGS.SAFE.PROPORTION" = "", ".Purse.Seine.JGS.SAFE.PROPORTION" = "", ".Longline.JGS.SAFE.PROPORTION" = ""))
} else {
  comms_only<- str_replace_all(names(foots_stack), c(".Pot.Trap.JGS.PROPORTION" = "", ".Other.JGS.PROPORTION" = "", ".Gillnet.JGS.PROPORTION" = "", ".Trawl.JGS.PROPORTION" = "", ".Dredge.JGS.PROPORTION" = "", ".Purse.Seine.JGS.PROPORTION" = "", ".Longline.JGS.PROPORTION" = ""))
}

comms_unique<- unique(comms_only) # 126 ports

all_stack<- stack()

for(i in seq_along(comms_unique)){
  comm_use<- comms_unique[i]
  comm_foot_ind<- which(grepl(comm_use, names(foots_stack)), arr.ind = T)
  stack_use<- foots_stack[[comm_foot_ind]]
  if(nlayers(stack_use) == 1){
    # If just one footprint for that community (e.g., one gear type), then can just store that as that is also the "all gear" footprint
    all_gear_out<- stack_use[[1]]
  } else {
    # If more than one footprint for that community (e.g., multiple gear types), we can take the sum and that should then allow us to identify any fished cells for that community
    all_gear_out<- calc(stack_use, sum, na.rm = T)
  }
  
  # Add it to our stack of all gear community footprints
  all_stack<- raster::stack(all_stack, all_gear_out)
  
  # Print loop iteration incase there is an issue
  print(paste("Iteration", i, "is done", sep = " "))
}

# Add the "all gear" footprints to the gear specific ones
if(foot_safe){
  names_all<- c(names_base, paste(comms_unique, ".All.JGS.SAFE.PROPORTION", sep = ""))
} else {
  names_all<- c(names_base, paste(comms_unique, ".All.JGS.PROPORTION", sep = ""))
}

# Check it before overwriting the "foots_stack" object
check_stack<- raster::stack(foots_stack, all_stack)
names(check_stack)<- names_all
print(check_stack)

# All looks good, overwrite it
foots_stack<- check_stack
```

**Calculating "availability" within community fishing footprints**
With the footprints cleaned up a bit, we can now think about overlaying these footprints with out model projections to localize the shelf-wide results to coastal communities. To start, we will need a new function. It would be nice to use something like the `shp_over_pts_func` function, but right now, that is limited to using shapefiles. Here, the fishing footprints are stored as raster layers. So, that won't work and we need something different.
```{r, eval = eval_use}
# Create the function
fish_avail_func<- function(dat) {
  
  if(FALSE){
    dat = mod_res$Proj_Baseline_Biomass[[1]]
  }
  
  # Convert data frame to sf and then a raster, with same resolution as the footprints
  dat_sf<- dat %>%
    st_as_sf(., coords = c("x", "y"), crs = 4326, remove = FALSE)
  dat_rast<- rasterize(dat_sf, foots_stack[[1]], field = "Pred", fun = mean, na.rm = TRUE)
  dat_rast<- raster::resample(dat_rast, foots_stack[[1]])
  
  # Create a storage vector for the mean and the SD for each of the footprints in foots_stack
  mean_out<- vector(length = raster::nlayers(foots_stack), mode = "double")
  mean_out[]<- NA
  sd_out<- vector(length = raster::nlayers(foots_stack), mode = "double")
  sd_out[]<- NA
  
  # Loop through each footprint, and then summarize the projected values within the footprint
  for(i in 1:raster::nlayers(foots_stack)) {
    
    # Specific layer we will be working with
    lay_use<- foots_stack[[i]]
    
    # If no data for that footprint, then mean and sd will be NA
    if(all(is.na(values(lay_use))) | cellStats(lay_use, sum, na.rm = TRUE) == 0){
      mean_out[i]<- NA
      sd_out[i]<- NA
    } else {
      # If there is data, just turn the fishing kept proportion information into binary 0/1, which will signal the cells to keep of the projection data
      m<- c(0, Inf, 1,  -Inf, 0, 0)
      rclmat<- matrix(m, ncol = 3, byrow = TRUE)
      lay_bin<- raster::reclassify(lay_use, rclmat)
      
      # Get coordinates of footprint cells that were fished and convert them to sf for extraction
      foot_coords<- data.frame(rasterToPoints(lay_bin, function(x) x == 1)) %>%
        st_as_sf(., coords = c("x", "y"), crs = 4326, remove = FALSE)
      
      # Okay, now get the projected values at those points and then the proportion of catch at each to use as the weights. 
      proj_vals<- raster::extract(dat_rast, foot_coords)
    
      # Mean and SD
      if(all(is.na(proj_vals))) {
        mean_out[i]<- NA
        sd_out[i]<- NA
      } else {
        if(length(proj_vals) == 1){
          # Can't get SD with one value
          mean_out[i]<- mean(proj_vals, na.rm = T)
          sd_out[i]<- NA
        } else {
          mean_out[i]<- mean(proj_vals, na.rm = T)
          sd_out[i]<- sd(proj_vals, na.rm = T)
        }
      }
    }
  }
  
  # Create a data frame with each community footprint name, and then the resulting mean SD of projected values
  out_df<- data.frame("Community_Footprint" = names(foots_stack), "Mean_Avail" = mean_out, "SD_Avail" = sd_out)
  return(out_df)
}

# Now map it to mod_res and the specific columns we are interested in...
cols_keep<- c("COMNAME", "SEASON", "Proj_Baseline_Biomass", "RCP45_2055_Mu_Biomass", "RCP85_2055_Mu_Biomass")
foot_res<- mod_res %>%
  select(., one_of(cols_keep)) %>%
  mutate(., "Baseline_Comm_Avail" = map(Proj_Baseline_Biomass, fish_avail_func),
         "RCP45_2055_Comm_Avail" = map(RCP45_2055_Mu_Biomass, fish_avail_func),
         "RCP85_2055_Comm_Avail" = map(RCP85_2055_Mu_Biomass, fish_avail_func))

# What did that just do?? For each species-season-community-footprint, we now have the mean and SD projected values 
summary(foot_res$RCP45_2055_Comm_Avail[[1]])
```

**Calculating changes in "availability" within community fishing footprints**
Alright, the final step is using the average projected values (baseline and under future potential conditions from the climate scenarios/ensemble statistic/time horizons) to get a percentage change in availability. Like before, we will write a function to do this and then map the function to the associated columns of our "foot_res" dataframe.
```{r, eval = eval_use}
# A change function, dealing with some of the subtleties for how the data are stored within the "foot_res" piece...
avail_change_func<- function(baseline, projected, scale){
  if(FALSE){
    baseline = foot_res$Baseline_Comm_Avail[[1]]
    projected = foot_res$RCP45_2055_Comm_Avail[[1]]
    scale = "percent"
  }
  
  avail_change_out<- data.frame("Community_Footprint" = baseline$Community_Footprint)
  
  if(scale == "raw"){
    avail_change_out$Change<- projected$Mean_Avail - baseline$Mean_Avail
    return(avail_change_out)
  } else {
    avail_change_out$Change<- 100*((projected$Mean_Avail - baseline$Mean_Avail)/baseline$Mean_Avail)
    return(avail_change_out)
  }
}

# Map it...
foot_res<- foot_res %>%
  mutate(., "Comm_RCP45_2055_PercChange" = pmap(list(baseline = Baseline_Comm_Avail, projected = RCP45_2055_Comm_Avail, scale = list("percent")), avail_change_func),
        "Comm_RCP85_2055_PercChange" = pmap(list(baseline = Baseline_Comm_Avail, projected = RCP85_2055_Comm_Avail, scale = list("percent")), avail_change_func))

# Look at it...
summary(foot_res$Comm_RCP45_2055_PercChange[[1]])
```

## Where to go from here?
Hopefully this has laid out our workflow for the COCA I project in a way that is at least *somewhat* easy to follow and understand. More importantly, I hope it has also highlighted areas where I think we can improve things -- and I am sure there are areas that I have overlooked that might jump out to you, too! Overall, it is great that we have something that "works", but I am excited to work together to make this workflow even more efficient and robust. 

```{r, echo = FALSE, eval = TRUE}
gmRi::insert_gmri_footer(footer_file = "aallyn_gmri_footer.html")
```
